{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Description\r\nThe goal of this project is to predict whether a given tweet is about a real disaster or not. \"For each ID in the test set, you must predict 1 if the tweet is describing a real disaster, and 0 otherwise.\" - Submission File section of the NLP disaster tweets project overview page on Kaggle.\r\n\r\n# Data\r\n- id: a unique identifier for each tweet\r\n- text: the text of the tweet\r\n- location: the location the tweet was sent from (may be blank)\r\n- keyword: a particular keyword from the tweet (may be blank)\r\n- target: in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\r\n\r\n# NLP description \r\nNLP (natural language processing) is a type of machine learning that allows computers to interpret text data.  \"A technology that uses machine learning algorithms to interpret human language and analyze data. NLP can be used to automate legal discovery, organize information, and speed up reviews. It can also be used to improve the user experience for voice assistants and generative AI chatbots.\" - Gemeni Output for the following prompt: \"nlp.ext data.  ","metadata":{}},{"cell_type":"markdown","source":"# Project Setup","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport warnings\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')       ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:03:28.543568Z","iopub.execute_input":"2024-12-08T00:03:28.544410Z","iopub.status.idle":"2024-12-08T00:03:41.369717Z","shell.execute_reply.started":"2024-12-08T00:03:28.544374Z","shell.execute_reply":"2024-12-08T00:03:41.368769Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nHere I looked at the basic structure of the data and looked for any missing values.  I plotted the target variable distribution as well as the word count distribution.","metadata":{}},{"cell_type":"code","source":"train_data.info()\nprint('')\nprint(train_data.head())\n\n# Missing values count\nmissing_values = train_data.isnull().sum()\nmissing_percentage = (missing_values / len(train_data)) * 100\nprint(\"\\nMissing Values:\\n\", pd.DataFrame({\"Count\": missing_values, \"Percentage\": missing_percentage}))\n\n# Target distribution\nsns.countplot(x='target', data=train_data)\nplt.title('Distribution of Target Variable')\nplt.xlabel('Not Disaster (0) / Disaster (1)')\nplt.ylabel('Count')\nplt.show()\n\n# Word count distribution\ntrain_data['word_count'] = train_data['text'].apply(lambda x: len(str(x).split()))\nsns.histplot(train_data['word_count'], kde=True)\nplt.title('Distribution of Word Count in Tweets')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:03:41.371359Z","iopub.execute_input":"2024-12-08T00:03:41.371945Z","iopub.status.idle":"2024-12-08T00:03:41.964829Z","shell.execute_reply.started":"2024-12-08T00:03:41.371904Z","shell.execute_reply":"2024-12-08T00:03:41.963968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plan of Analysis \r\nLooking at the distribution of the target variable, it seems that it is relatively-evenly balanced.  This will allow for standard classification modeling without adjustments for class imbalance.  The following code will precede the modeling phase. I will be applying tolkenization (splitting text into units), removing stop words (excluding any words that are common and provide little information), sequencing and padding (ordering words and ensuring common lengths), and splitting the data into train and test sets (validation).  I then will be training it using an LSTM (Long-Short Term Memory Recurrent Neural Network). ","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning/Preprocessing\r\nHere I removed stop words and split out the features from the target variable.  I initilized an out-of-vocabulary tolkenizer for unseen words.  I transformed the text into a sequence of numbers based off the tolkenizer index. I padded the text with zeroes on the end of shorter sequences ('post' argument). I then split out the training data from the test data. ","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n\n# separate features and remove stop words\nX = train_data['text'].apply(lambda x: remove_stopwords(str(x))).values\ny = train_data['target'].values\n\nvocab_size = 10000\nembedding_dim = 64\n\n# tolkenizer init\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X)\n\n# sequence and pad\nX_sequences = tokenizer.texts_to_sequences(X)\nX_padded = pad_sequences(X_sequences, padding='post')\n\n# split data\nX_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:03:41.966123Z","iopub.execute_input":"2024-12-08T00:03:41.966806Z","iopub.status.idle":"2024-12-08T00:03:43.032714Z","shell.execute_reply.started":"2024-12-08T00:03:41.966762Z","shell.execute_reply":"2024-12-08T00:03:43.031998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture\r\n- Model structure and compilation\r\n- Instantiate callbacks\r\n- Train the model\r\n- Hyper parameter tuning for the number of LSTM Units","metadata":{}},{"cell_type":"code","source":"results = []\n\nfor units in [32, 64, 128]:\n  print(f\"Training model with {units} LSTM units...\")\n\n  model = Sequential([\n      Embedding(vocab_size, embedding_dim),\n      LSTM(units, dropout=0.2, recurrent_dropout=0.2),\n      Dense(32, activation='relu'),\n      Dropout(0.5),\n      Dense(1, activation='sigmoid')\n  ])\n\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n  early_stopping = EarlyStopping(\n      monitor='val_accuracy', \n      patience=3,  \n      restore_best_weights=True \n  )\n\n  model_checkpoint = ModelCheckpoint(\n      'best_model.keras',  \n      monitor='val_loss',\n      save_best_only=True,  \n      mode='min' \n  )\n\n  history = model.fit(\n      X_train, y_train,\n      validation_data=(X_val, y_val),\n      epochs=50,\n      batch_size=64,\n      verbose=2,\n      callbacks=[early_stopping, model_checkpoint]\n  )\n  best_val_accuracy = max(history.history['val_accuracy'])\n  best_val_loss = min(history.history['val_loss'])\n  results.append({'LSTM Units': units, \n                  'Best Val Accuracy': best_val_accuracy, \n                  'Best Val Loss': best_val_loss})\n\nresults_df = pd.DataFrame(results)\nprint(\"\\nSummary of Hyperparameter Search Results:\")\nprint(results_df)\n\nprint(f\"\\nModel with {units} LSTM units achieved best validation accuracy: {best_val_accuracy:.4f}\")\nprint(f\"Model with {units} LSTM units achieved best validation loss: {best_val_loss:.4f}\")\n\nprint('\\nTraining Complete')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:05:38.249955Z","iopub.execute_input":"2024-12-08T00:05:38.250613Z","iopub.status.idle":"2024-12-08T00:07:01.301674Z","shell.execute_reply.started":"2024-12-08T00:05:38.250578Z","shell.execute_reply":"2024-12-08T00:07:01.300688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results and Analysis \rThe final model achieved the following metrics on the validation set:\n\n- Validation Accuracy: 0.80\n- Precision (Non-Disaster): 0.80\n- Recall (Non-Disaster): 0.88\n- F1-Score (Non-Disaster): 0.83\n- Precision (Disaster): 0.81\n- Recall (Disaster): 0.69\n- F1-Score (Disaster): 0.75.\n### Hyperparameter Tuning Process\n\n| **LSTM Units** | **Best Val Accuracy** | **Best Val Loss** |\n|----------------|-----------------------|-------------------|\n| 32             | 0.8049               | 0.4617           |\n| 64             | 0.8004               | 0.4673           |\n| 128            | 0.8004               | 0.4619           |\n\r\n\r\n# Conclusion\r\nThe model had generally good performance. The model identifies negative cases correctly. The model generally did well with the postitives but missed more than the negative class.  This could be an issue in the usefulness of the model where detecting disaster tweets may be a bigger priority than limiting false positives. Something that could be done is to tune the model for better recall if the priority is detecting positive cases.  Hyperparameter tuning and layer engineering could be an area to improve on and increase the score further.  Preprocessing the data further with techniques like lemmatization or stemming might be helpful as w","metadata":{}},{"cell_type":"code","source":"val_loss, val_accuracy = model.evaluate(X_val, y_val)\nprint(f'Validation Accuracy: {val_accuracy:.2f}')\n\ny_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\nprint(classification_report(y_val, y_pred, zero_division=1))\n\n\n# Plot accuracy\nplt.figure(figsize=(14, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:22:46.727259Z","iopub.execute_input":"2024-12-08T00:22:46.727590Z","iopub.status.idle":"2024-12-08T00:22:48.573414Z","shell.execute_reply.started":"2024-12-08T00:22:46.727564Z","shell.execute_reply":"2024-12-08T00:22:48.572586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nX_test = test_data['text'].values\n\nX_test_sequences = tokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(X_test_sequences, padding='post')\n\npredictions = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\n\nsubmission = pd.DataFrame({\n    'id': test_data['id'],\n    'target': predictions.flatten()\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:04:58.314634Z","iopub.execute_input":"2024-12-08T00:04:58.314875Z","iopub.status.idle":"2024-12-08T00:05:00.042888Z","shell.execute_reply.started":"2024-12-08T00:04:58.314852Z","shell.execute_reply":"2024-12-08T00:05:00.042238Z"}},"outputs":[],"execution_count":null}]}